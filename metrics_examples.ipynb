{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates three key concepts of ROUGE:\n",
    "\n",
    "- ROUGE-1: Measures unigram (single word) overlap. \n",
    "- ROUGE-2: Measures bigram (two consecutive words) overlap. \n",
    "- ROUGE-L: Measures longest common subsequence. \n",
    "The code shows three scenarios:\n",
    "\n",
    "Perfect match (scores will be 1.0). \n",
    "Partial match (scores will be between 0 and 1). \n",
    "Poor match (scores will be close to 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Perfect Match\n",
      "Reference: The cat sat on the mat.\n",
      "Candidate: The cat sat on the mat.\n",
      "ROUGE Scores:\n",
      "ROUGE-1: 1.000\n",
      "ROUGE-2: 1.000\n",
      "ROUGE-L: 1.000\n",
      "\n",
      "\n",
      "Example 2: Partial Match\n",
      "Reference: The cat sat on the mat.\n",
      "Candidate: A cat is sitting on the mat.\n",
      "ROUGE Scores:\n",
      "ROUGE-1: 0.615\n",
      "ROUGE-2: 0.364\n",
      "ROUGE-L: 0.615\n",
      "\n",
      "\n",
      "Example 3: Poor Match\n",
      "Reference: The cat sat on the mat.\n",
      "Candidate: The dog ran in the yard.\n",
      "ROUGE Scores:\n",
      "ROUGE-1: 0.333\n",
      "ROUGE-2: 0.000\n",
      "ROUGE-L: 0.333\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "# Initialize ROUGE scorer with different ROUGE variants\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Example 1: Perfect match\n",
    "reference = \"The cat sat on the mat.\"\n",
    "candidate = \"The cat sat on the mat.\"\n",
    "\n",
    "print(\"Example 1: Perfect Match\")\n",
    "print(f\"Reference: {reference}\")\n",
    "print(f\"Candidate: {candidate}\")\n",
    "scores = scorer.score(reference, candidate)\n",
    "print(\"ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {scores['rouge1'].fmeasure:.3f}\")\n",
    "print(f\"ROUGE-2: {scores['rouge2'].fmeasure:.3f}\")\n",
    "print(f\"ROUGE-L: {scores['rougeL'].fmeasure:.3f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Example 2: Partial match\n",
    "reference = \"The cat sat on the mat.\"\n",
    "candidate = \"A cat is sitting on the mat.\"\n",
    "\n",
    "print(\"Example 2: Partial Match\")\n",
    "print(f\"Reference: {reference}\")\n",
    "print(f\"Candidate: {candidate}\")\n",
    "scores = scorer.score(reference, candidate)\n",
    "print(\"ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {scores['rouge1'].fmeasure:.3f}\")\n",
    "print(f\"ROUGE-2: {scores['rouge2'].fmeasure:.3f}\")\n",
    "print(f\"ROUGE-L: {scores['rougeL'].fmeasure:.3f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Example 3: Poor match\n",
    "reference = \"The cat sat on the mat.\"\n",
    "candidate = \"The dog ran in the yard.\"\n",
    "\n",
    "print(\"Example 3: Poor Match\")\n",
    "print(f\"Reference: {reference}\")\n",
    "print(f\"Candidate: {candidate}\")\n",
    "scores = scorer.score(reference, candidate)\n",
    "print(\"ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {scores['rouge1'].fmeasure:.3f}\")\n",
    "print(f\"ROUGE-2: {scores['rouge2'].fmeasure:.3f}\")\n",
    "print(f\"ROUGE-L: {scores['rougeL'].fmeasure:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic similarity metric - BERT SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example demonstrates three key aspects of BERTScore:  \n",
    "\n",
    "Perfect Match:  \n",
    "Same sentences should get very high scores (close to 1.0). \n",
    "Shows baseline for perfect similarity\n",
    "\n",
    "Partial Match:  \n",
    "Semantically similar but different wording. \n",
    "Demonstrates BERTScore's ability to capture meaning beyond exact matches. \n",
    "\n",
    "Poor Match:  \n",
    "Different meaning and words. \n",
    "Shows how scores decrease with semantic dissimilarity. \n",
    "\n",
    "### The output includes:  \n",
    "- Precision: How well candidate words match reference\n",
    "- Recall: How well reference words are captured in candidate\n",
    "- F1: Harmonic mean of precision and recall\n",
    "\n",
    "### Key differences from ROUGE:  \n",
    "- BERTScore uses contextual embeddings, capturing semantic similarity\n",
    "- Can identify similar meanings even with different words\n",
    "- More robust to paraphrasing than ROUGE\n",
    "\n",
    "BERTScore typically correlates better with human judgment than ROUGE because it captures semantic similarity rather than just lexical overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Reference: The cat sat on the mat.\n",
      "Candidate: The cat sat on the mat.\n",
      "BERTScore metrics:\n",
      "Precision: 1.000\n",
      "Recall: 1.000\n",
      "F1: 1.000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Reference: The cat sat on the mat.\n",
      "Candidate: A cat is sitting on the mat.\n",
      "BERTScore metrics:\n",
      "Precision: 0.805\n",
      "Recall: 0.859\n",
      "F1: 0.832\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 3:\n",
      "Reference: The cat sat on the mat.\n",
      "Candidate: The dog ran in the yard.\n",
      "BERTScore metrics:\n",
      "Precision: 0.683\n",
      "Recall: 0.608\n",
      "F1: 0.646\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 4:\n",
      "Reference: The cat sat on the mat.\n",
      "Candidate: The stock market crashed yesterday.\n",
      "BERTScore metrics:\n",
      "Precision: 0.247\n",
      "Recall: 0.293\n",
      "F1: 0.271\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "import torch\n",
    "\n",
    "# Ensure using CPU if no GPU available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Set rescale_with_baseline=True for more interpretable scores\n",
    "torch.hub.set_dir('./torch_cache')  # Optional: set cache directory\n",
    "\n",
    "examples = [\n",
    "    # Example 1: Perfect match\n",
    "    {\n",
    "        \"reference\": \"The cat sat on the mat.\",\n",
    "        \"candidate\": \"The cat sat on the mat.\"\n",
    "    },\n",
    "    # Example 2: Semantically similar\n",
    "    {\n",
    "        \"reference\": \"The cat sat on the mat.\",\n",
    "        \"candidate\": \"A cat is sitting on the mat.\"\n",
    "    },\n",
    "    # Example 3: Different but related concepts\n",
    "    {\n",
    "        \"reference\": \"The cat sat on the mat.\",\n",
    "        \"candidate\": \"The dog ran in the yard.\"\n",
    "    },\n",
    "    # Example 4: Completely different meaning\n",
    "    {\n",
    "        \"reference\": \"The cat sat on the mat.\",\n",
    "        \"candidate\": \"The stock market crashed yesterday.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, example in enumerate(examples, 1):\n",
    "    P, R, F1 = score([example[\"candidate\"]], \n",
    "                    [example[\"reference\"]], \n",
    "                    lang=\"en\", \n",
    "                    rescale_with_baseline=True,\n",
    "                    model_type=\"roberta-large\")\n",
    "    \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Reference: {example['reference']}\")\n",
    "    print(f\"Candidate: {example['candidate']}\")\n",
    "    print(\"BERTScore metrics:\")\n",
    "    print(f\"Precision: {P.mean():.3f}\")\n",
    "    print(f\"Recall: {R.mean():.3f}\")\n",
    "    print(f\"F1: {F1.mean():.3f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic vs Textual - When to Use Each Type of Metric\n",
    "\n",
    "## Key Decision Factors:\n",
    "- Precision Critical → ROUGE\n",
    "- Meaning Critical → BERTScore\n",
    "- Multiple Valid Expressions → BERTScore\n",
    "- Exact Terminology Required → ROUGE\n",
    "\n",
    "## ROUGE (Textual/N-gram Based) use cases:\n",
    "\n",
    "### 1. News Headline Generation\n",
    "- Need exact terminology preservation\n",
    "- Key facts and names must match precisely\n",
    "- Example: \"Apple launches iPhone 15\" must contain exact product names\n",
    "\n",
    "### 2. Medical Report Summarization\n",
    "- Critical medical terms must be preserved\n",
    "- No room for semantic alternatives\n",
    "- Example: \"Patient shows signs of hypertension\" vs \"Patient has high blood pressure\"\n",
    "\n",
    "### 3. Legal Document Summarization\n",
    "- Specific legal terminology must be maintained\n",
    "- Exact phrasing is crucial\n",
    "- Example: Contract terms and conditions\n",
    "\n",
    "## BERTScore (Semantic) use cases:\n",
    "\n",
    "### 1. Customer Review Summarization\n",
    "- \"The food was excellent\" ≈ \"The meal was fantastic\"\n",
    "- Capturing sentiment is more important than exact wording\n",
    "- Example: Amazon product review summaries\n",
    "\n",
    "### 2. Conversational AI Responses\n",
    "- Multiple valid ways to express same information\n",
    "- Focus on meaning rather than exact wording\n",
    "- Example: Chatbot responses\n",
    "\n",
    "### 3. Multi-lingual Translation Evaluation\n",
    "- Same meaning in different languages\n",
    "- Accounts for cultural and linguistic variations\n",
    "- Example: Evaluating machine translation quality\n",
    "\n",
    "### 4. Social Media Content Analysis\n",
    "- Similar ideas expressed in different ways\n",
    "- Slang and variations are common\n",
    "- Example: Twitter sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
