{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Nova.lite and Nova.pro on MeetingBank Dataset\n",
    "\n",
    "This notebook demonstrates how to evaluate Amazon Bedrock models (Nova.lite and Nova.pro) on the MeetingBank dataset for meeting summarization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "# Import utility functions\n",
    "from utils.dataset_utils import load_meetingbank_dataset, get_test_samples, prepare_for_bedrock_evaluation\n",
    "from utils.bedrock_utils import (\n",
    "    create_s3_bucket_if_not_exists,\n",
    "    apply_cors_if_not_exists,\n",
    "    upload_to_s3,\n",
    "    create_evaluation_job,\n",
    "    wait_for_job_completion,\n",
    "    download_evaluation_results,\n",
    "    analyze_evaluation_results,\n",
    "    visualize_evaluation_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure AWS Credentials\n",
    "\n",
    "Make sure you have AWS credentials configured with appropriate permissions for Amazon Bedrock and S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set AWS region\n",
    "region = \"us-east-1\"  # Change to your preferred region where Bedrock is available\n",
    "BEDROCK_ROLE_ARN = \"arn:aws:iam::864016358360:role/service-role/Amazon-Bedrock-IAM-Role-20250531T202875\"\n",
    "bucket_name = 'eval-datasets-us-east-1'\n",
    "NUM_SAMPLES_PER_EVAL = 862\n",
    "\n",
    "# Set IAM role ARN with permissions for Bedrock evaluation\n",
    "# This role needs permissions to access S3 and invoke Bedrock models\n",
    "#os.environ[\"BEDROCK_ROLE_ARN\"] = \"arn:aws:iam::YOUR_ACCOUNT_ID:role/YOUR_BEDROCK_ROLE\"  # Replace with your role ARN\n",
    "os.environ[\"BEDROCK_ROLE_ARN\"] = BEDROCK_ROLE_ARN\n",
    "\n",
    "# Verify AWS credentials\n",
    "try:\n",
    "    sts = boto3.client('sts')\n",
    "    identity = sts.get_caller_identity()\n",
    "    print(f\"AWS Identity verified: {identity['Arn']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error verifying AWS credentials: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load MeetingBank Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_meetingbank_dataset()\n",
    "print(f\"Dataset structure: {dataset}\")\n",
    "print(f\"Available splits: {dataset.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first {NUM_SAMPLES_PER_EVAL} samples from the test set\n",
    "test_samples = get_test_samples(dataset, num_samples=NUM_SAMPLES_PER_EVAL)\n",
    "print(f\"Number of test samples: {len(test_samples)}\")\n",
    "\n",
    "# Display sample information\n",
    "for i, sample in enumerate(test_samples[:5]):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Transcript length: {len(sample['transcript'])} characters\")\n",
    "    print(f\"Summary length: {len(sample['summary'])} characters\")\n",
    "    print(f\"Summary: {sample['summary'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset for Bedrock Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for Bedrock evaluation\n",
    "evaluation_dataset_path = prepare_for_bedrock_evaluation(test_samples)\n",
    "print(f\"Evaluation dataset created at: {evaluation_dataset_path}\")\n",
    "\n",
    "# Display the content of the evaluation dataset\n",
    "with open(evaluation_dataset_path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:  # Only show first 5 records\n",
    "            break\n",
    "        record = json.loads(line)\n",
    "        print(f\"\\nRecord {i+1}:\")\n",
    "        print(f\"Prompt length: {len(record['prompt'])} characters\")\n",
    "        print(f\"Reference response length: {len(record['referenceResponse'])} characters\")\n",
    "        print(f\"Category: {record['category']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload Dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_s3_bucket_if_not_exists(bucket_name, region)\n",
    "apply_cors_if_not_exists(bucket_name, region)\n",
    "\n",
    "# Upload the evaluation dataset to S3\n",
    "dataset_s3_key = \"evaluation/meetingbank_dataset.jsonl\"\n",
    "dataset_s3_uri = upload_to_s3(evaluation_dataset_path, bucket_name, dataset_s3_key, region)\n",
    "print(f\"Dataset uploaded to: {dataset_s3_uri}\")\n",
    "\n",
    "# Define the output location in S3\n",
    "output_s3_uri = f\"s3://{bucket_name}/evaluation/results/\"\n",
    "print(f\"Results will be stored at: {output_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create and Run Bedrock Evaluation Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the models to evaluate\n",
    "models = [\n",
    "    {\n",
    "        \"name\" : \"nova-micro\",\n",
    "        \"model_id\" : \"us.amazon.nova-micro-v1:0\",\n",
    "    },\n",
    "    {\n",
    "        \"name\" : \"nova-lite\",\n",
    "        \"model_id\" : \"us.amazon.nova-lite-v1:0\",\n",
    "    },\n",
    "    {\n",
    "        \"name\" : \"nova-pro\",\n",
    "        \"model_id\" : \"us.amazon.nova-pro-v1:0\",\n",
    "    },\n",
    "    {\n",
    "        \"name\" : \"nova-premier\",\n",
    "        \"model_id\" : \"us.amazon.nova-premier-v1:0\",\n",
    "    },\n",
    "    {\n",
    "        \"name\" : \"haiku-3\",\n",
    "        \"model_id\" : \"us.anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    },\n",
    "    {\n",
    "        \"name\" : \"sonnet-3-5-v2\",\n",
    "        \"model_id\" : \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    print(f\"Model: {model}\")\n",
    "    model[\"name\"] = model[\"name\"].lower()\n",
    "    model_name = model[\"name\"]\n",
    "    model_id = model[\"model_id\"]\n",
    "    # Create a unique job name\n",
    "    job_name = f\"meetingbank-{model_name}-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "    model['job_name'] = job_name\n",
    "\n",
    "    # Create the evaluation job\n",
    "    try:\n",
    "        job_arn = create_evaluation_job(\n",
    "            job_name=job_name,\n",
    "            dataset_s3_uri=dataset_s3_uri,\n",
    "            output_s3_uri=output_s3_uri,\n",
    "            model_id=model_id,\n",
    "            region=region\n",
    "        )\n",
    "        print(f\"Evaluation job created with ARN: {job_arn}\")\n",
    "        model['job_arn'] = job_arn\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating evaluation job: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    job_arn=model['job_arn']\n",
    "    print(f\"name = {model[\"name\"]}. Job ARN: {job_arn}\")\n",
    "    # Wait for the job to complete\n",
    "    print(\"Waiting for evaluation job to complete...\")\n",
    "    job_details = wait_for_job_completion(job_arn, region)\n",
    "    print(f\"Job completed with status: {job_details['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = './results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "for model in models:\n",
    "    job_name = model['job_name']\n",
    "    results_local_path = f'{results_dir}/{job_name}'\n",
    "    # Download the evaluation results\n",
    "    results_base_dir_s3 = f\"{output_s3_uri}/{job_name}/\"\n",
    "    print(f\"Downloading results for {model['name']} from {results_base_dir_s3}\")\n",
    "\n",
    "    try:\n",
    "        download_evaluation_results(results_base_dir_s3, results_local_path, region)\n",
    "        print(f\"Results downloaded to: {results_local_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading results: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the evaluation results\n",
    "print(\"Analyzing evaluation results...\")\n",
    "results_df = analyze_evaluation_results(results_dir)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "print(\"Creating visualization...\")\n",
    "ax = visualize_evaluation_results(results_df, output_path='evaluation_results.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "1. Load the MeetingBank dataset\n",
    "2. Prepare the dataset for Bedrock evaluation\n",
    "3. Create and run a Bedrock evaluation job\n",
    "4. Analyze and visualize the evaluation results\n",
    "\n",
    "The evaluation compared Nova models and Claude models on meeting summarization tasks using built-in Bedrock evaluators for Relevance, Correctness, Completeness, and Coherence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}